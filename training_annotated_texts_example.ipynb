{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47cf01ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-11 16:32:52.941219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-11 16:32:53.135868: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-11 16:32:53.135929: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-11 16:33:02.368310: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-11 16:33:02.368491: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-11 16:33:02.368512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import XLMRobertaForTokenClassification, XLMRobertaTokenizerFast, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_metric\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45cc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "#unmasker = pipeline('ner', model='xlm-roberta-base')\n",
    "#unmasker(\"Hello I'm a <mask> model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd38c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_training_data_proc = [\"../../gpt2/nov25datafortraining/test_gpt4_498_procedures_from_500_azure.tsv\", \"../../gpt2/nov25datafortraining/test_gpt4_500_procedures_from_500to1000_azure.tsv\", \"../../gpt2/nov25datafortraining/test_gpt4_363_procedures_from_1363_azure.tsv\"]\n",
    "list_of_training_data_anamn = [\"../../gpt2/nov25datafortraining/test_gpt4_852_anamnesis_from_854_azure.tsv\", \"../../gpt2/nov25datafortraining/test_gpt4_642_anamnesis_from_645_azure.tsv\", \"../../gpt2/nov25datafortraining/test_gpt4_465_anamnesis_from_466_azure.tsv\", \"../../gpt2/nov25datafortraining/test_gpt4_800_anamnesis_from_801_azure.tsv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "485b2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "output_proc = []\n",
    "output_anamnesis = []\n",
    "\n",
    "for proc_train in list_of_training_data_proc:\n",
    "    with open(proc_train, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "        output_proc += list(reader)\n",
    "        \n",
    "for anamn_train in list_of_training_data_anamn:\n",
    "    with open(anamn_train, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "        output_anamnesis += list(reader)\n",
    "    \n",
    "with open('../../gpt2/testset_maria_annotated.tsv', 'r') as csvfile:\n",
    "    reader2 = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    output_test = list(reader2)\n",
    "     \n",
    "\n",
    "output_proc = [row for row in output_proc]\n",
    "output_anamnesis = [row for row in output_anamnesis]\n",
    "output_test = [row for row in output_test]\n",
    "\n",
    "# labeled_tokens = [(token, label) for token, label in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fbe0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_anam = []\n",
    "temporary_anam = []\n",
    "\n",
    "total_proc = []\n",
    "temporary_proc = []\n",
    "\n",
    "\n",
    "for entry in output_proc:\n",
    "    if len(entry) == 2:\n",
    "        text = entry[0]\n",
    "        label = entry[1]\n",
    "        if label == '0':\n",
    "            label = 'O'\n",
    "        if text not in ['SEP', 'CLS']:\n",
    "            temporary_proc.append([text, label])\n",
    "    if len(entry) < 2:\n",
    "        total_proc.append(temporary_proc)\n",
    "        temporary_proc = []\n",
    "        \n",
    "for entry in output_anamnesis:\n",
    "    if len(entry) == 2:\n",
    "        text = entry[0]\n",
    "        label = entry[1]\n",
    "        if label == '0':\n",
    "            label = 'O'\n",
    "        if text not in ['SEP', 'CLS']:\n",
    "            temporary_anam.append([text, label])\n",
    "    if len(entry) < 2:\n",
    "        total_anam.append(temporary_anam)\n",
    "        temporary_anam = []\n",
    "        \n",
    "total_test = []\n",
    "temporary_test = []\n",
    "\n",
    "for entry in output_test:\n",
    "    if len(entry) == 2:\n",
    "        text = entry[0]\n",
    "        label = entry[1]\n",
    "        \n",
    "        if label in ['0', 'BREAST']:\n",
    "            label = 'O'\n",
    "        if text not in ['SEP', 'CLS']:\n",
    "            temporary_test.append([text, label])\n",
    "    if len(entry) < 2:\n",
    "        total_test.append(temporary_test)\n",
    "        temporary_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68cfeb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "# first batch\n",
    "\n",
    "total = total_anam[:167] + total_proc[:83]\n",
    "print(len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e444011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(total))\n",
    "print(len(total_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "583257ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunking(data):\n",
    "    prev_tag = None\n",
    "    all_data = []\n",
    "    for entry in data:\n",
    "        new_data = []\n",
    "        for word, tag in entry:\n",
    "            if tag != 'ADE':\n",
    "                tag = tag.capitalize()\n",
    "            if tag == 'O':\n",
    "                new_data.append((word, tag))\n",
    "                prev_tag = None\n",
    "            elif prev_tag == tag:\n",
    "                new_data.append((word, 'I-' + tag))\n",
    "            else:\n",
    "                new_data.append((word, 'B-' + tag))\n",
    "                prev_tag = tag\n",
    "        all_data.append(new_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04efdeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "250\n",
      "\n",
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(total))\n",
    "total = add_chunking(total)\n",
    "print(len(total))\n",
    "print()\n",
    "print(len(total_test))\n",
    "total_test = add_chunking(total_test)\n",
    "print(len(total_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be96c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ = []\n",
    "labels_test_ = []\n",
    "\n",
    "for x in total:\n",
    "    for y in x:\n",
    "        label_ = y[1]\n",
    "        labels_.append(label_)\n",
    "        \n",
    "for x in total_test:\n",
    "    for y in x:\n",
    "        label_test = y[1]\n",
    "        labels_test_.append(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0551a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplabels = np.array(labels_)\n",
    "nplabels_test = np.array(labels_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d291bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = np.unique(nplabels, return_counts=True)\n",
    "unique_values_test = np.unique(nplabels_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfd23b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "B-Drug - 273\n",
      "B-Family - 35\n",
      "B-Procedure - 778\n",
      "I-Drug - 118\n",
      "I-Family - 39\n",
      "I-Procedure - 994\n",
      "O - 19168\n",
      "\n",
      "test:\n",
      "B-Drug - 347\n",
      "B-Family - 22\n",
      "B-Procedure - 572\n",
      "I-Drug - 309\n",
      "I-Family - 80\n",
      "I-Procedure - 612\n",
      "O - 38959\n"
     ]
    }
   ],
   "source": [
    "counts_ = unique_values[1]\n",
    "names_ = unique_values[0]\n",
    "\n",
    "counts_test_ = unique_values_test[1]\n",
    "names_test_ = unique_values_test[0]\n",
    "\n",
    "print(\"train:\")\n",
    "for k in range(len(counts_)):\n",
    "    print(names_[k], \"-\", counts_[k])\n",
    "\n",
    "print()\n",
    "print(\"test:\")\n",
    "for k in range(len(counts_test_)):\n",
    "    print(names_test_[k], \"-\", counts_test_[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f53d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(names_)\n",
    "label_list_test = list(names_test_)\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "label2id_test = {label: i for i, label in enumerate(label_list_test)}\n",
    "id2label_test = {i: label for i, label in enumerate(label_list_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e644063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Drug', 'B-Family', 'B-Procedure', 'I-Drug', 'I-Family', 'I-Procedure', 'O']\n",
      "['B-Drug', 'B-Family', 'B-Procedure', 'I-Drug', 'I-Family', 'I-Procedure', 'O']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(label_list)\n",
    "print(label_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fa8d14a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the XLM-RoBERTa model and tokenizer\n",
    "# label2id = {label: i for i, label in enumerate(label_list)}\n",
    "# id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "model = XLMRobertaForTokenClassification.from_pretrained('xlm-roberta-base', num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base', max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ac8d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = int((len(total) - int(len(total)*0.85))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91359e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anamn_val_batch = total[0:val_batch]\n",
    "proc_val_batch = total[400:400+val_batch]\n",
    "\n",
    "train_batch = total[val_batch:400] + total[400+val_batch:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d684dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anamn_val_batch + proc_val_batch + train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3cd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "297c29bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_70 = int(len(all_data) * 0.7)\n",
    "# split_85 = int(len(all_data) * 0.85)\n",
    "\n",
    "# training_total = all_data[:split_70]\n",
    "# validation_total = all_data[split_70:split_85]\n",
    "\n",
    "validation_total = anamn_val_batch + proc_val_batch\n",
    "training_total = train_batch\n",
    "test_total = total_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b197e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "19\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(training_total))\n",
    "print(len(validation_total))\n",
    "print(len(test_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00134c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pt', 'O'),\n",
       " ('soovib', 'O'),\n",
       " ('teha', 'O'),\n",
       " ('ct-uuringut', 'B-Procedure'),\n",
       " ('rindkerest', 'I-Procedure'),\n",
       " (',', 'O'),\n",
       " ('kÃµhust', 'B-Procedure'),\n",
       " ('ja', 'O'),\n",
       " ('vaagnast', 'B-Procedure'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_total[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5fb7e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def get_all_tokens_and_ner_tags(texts):\n",
    "    return pd.concat([get_tokens_and_ner_tags(texts)]).reset_index().drop('index', axis=1)\n",
    "    \n",
    "def get_tokens_and_ner_tags(texts):\n",
    "    all_tokens = []\n",
    "    all_entities = []\n",
    "    for row in texts:\n",
    "        tokens = [x[0] for x in row]\n",
    "        entities = [x[1] for x in row]\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_entities.append(entities)\n",
    "    return pd.DataFrame({'tokens': all_tokens, 'ner_tags': all_entities})\n",
    "\n",
    "def create_sliding_windows(train_dataset, max_length=512, stride=512):\n",
    "    windowed_dataset = []\n",
    "    for data in train_dataset:\n",
    "        tokens = data[\"tokens\"]\n",
    "        ner_tags = data[\"ner_tags\"]\n",
    "        for i in range(0, len(tokens), stride):\n",
    "            window_tokens = tokens[i:i+max_length]\n",
    "            window_ner_tags = ner_tags[i:i+max_length]\n",
    "            windowed_data = {\"tokens\": window_tokens, \"ner_tags\": window_ner_tags}\n",
    "            windowed_dataset.append(windowed_data)\n",
    "    windowss=pd.DataFrame(windowed_dataset)\n",
    "    return Dataset.from_pandas(windowss)\n",
    "\n",
    "def get_un_token_dataset(train_directory, test_directory, validation_directory):\n",
    "    train_df = get_all_tokens_and_ner_tags(train_directory)\n",
    "    test_df = get_all_tokens_and_ner_tags(test_directory)\n",
    "    validation_df = get_all_tokens_and_ner_tags(validation_directory)\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    validation_dataset = Dataset.from_pandas(validation_df)\n",
    "    \n",
    "    # Apply sliding window on train_dataset\n",
    "    train_dataset = create_sliding_windows(train_dataset)\n",
    "    test_dataset = create_sliding_windows(test_dataset)\n",
    "    validation_dataset = create_sliding_windows(validation_dataset)\n",
    "    \n",
    "    return (train_dataset, test_dataset, validation_dataset)\n",
    "\n",
    "train_dataset, test_dataset, validation_dataset = get_un_token_dataset(training_total, test_total, validation_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "902fc9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "300\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12441383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['pt',\n",
       "  'soovib',\n",
       "  'teha',\n",
       "  'ct-uuringut',\n",
       "  'rindkerest',\n",
       "  ',',\n",
       "  'kÃµhust',\n",
       "  'ja',\n",
       "  'vaagnast',\n",
       "  '.'],\n",
       " 'ner_tags': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-Procedure',\n",
       "  'I-Procedure',\n",
       "  'O',\n",
       "  'B-Procedure',\n",
       "  'O',\n",
       "  'B-Procedure',\n",
       "  'O']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "112a3a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['NAME', ':', 'DATE.', 'rinna', 'op'], 'ner_tags': ['O', 'O', 'O', 'B-Procedure', 'I-Procedure']}\n",
      "\n",
      "{'tokens': ['kiiritusravi', 'planeerimine', ',', 'ravi', 'selgitatud', ',', 'patsient', 'raviga', 'nÃµus', ',', 'allkirjastab', 'nÃµusoleku.', 'DATE.', 'alustab', 'raviga', ',', 'nahahooldus.kiiritusravi', 'talutava', 'probleemideta', ',', 'jÃ¤tkab', 'samal', 'viisil.', 'DATE.', 'lÃµpetab', 'raviga'], 'ner_tags': ['B-Procedure', 'I-Procedure', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Procedure', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "\n",
      "{'tokens': ['Perearst', 'saadab', 'Ãµhupuudusega', ',', 'teadmata', 'ajast', 'FA', 'teke', ',', 'III', 'korruse', 'minnes', 'Ãµhupuudus.', 'Stenokarilisi', 'kaebusi', 'otseselt', 'ei', 'ole.pÃµdes', 'astmat', '2', 'a.-t', 'LÃµpetas', 'suitsetamise', '4a.-t', 'tagasi.Allergia', 'taimedele', ',', 'anamneesi', 'annab', 'rÃ¼tmihÃ¤irete', 'osas', 'juba', '20', 'aastat.', 'Alustame', 'marevaniseerimist.', 'INR', 'hoida', 'perearstil', '2-3', 'vahel', ',', 'peale', 'seda', 'kavas', '4', 'nÃ¤dala', 'pÃ¤rast', 'kardioversioon.Kavas', 'EHHO', 'ja', 'rÃ¼tmi', 'Holter', '.'], 'ner_tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Drug', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Procedure', 'I-Procedure', 'O', 'O', 'B-Procedure', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print()\n",
    "print(validation_dataset[0])\n",
    "print()\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9f9332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-Drug': 0, 'B-Family': 1, 'B-Procedure': 2, 'I-Drug': 3, 'I-Family': 4, 'I-Procedure': 5, 'O': 6}\n",
      "{'B-Drug': 0, 'B-Family': 1, 'B-Procedure': 2, 'I-Drug': 3, 'I-Family': 4, 'I-Procedure': 5, 'O': 6}\n",
      "{0: 'B-Drug', 1: 'B-Family', 2: 'B-Procedure', 3: 'I-Drug', 4: 'I-Family', 5: 'I-Procedure', 6: 'O'}\n",
      "{0: 'B-Drug', 1: 'B-Family', 2: 'B-Procedure', 3: 'I-Drug', 4: 'I-Family', 5: 'I-Procedure', 6: 'O'}\n"
     ]
    }
   ],
   "source": [
    "print(label2id)\n",
    "print(label2id_test)\n",
    "\n",
    "print(id2label)\n",
    "print(id2label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89193a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-Drug': 0, 'B-Family': 1, 'B-Procedure': 2, 'I-Drug': 3, 'I-Family': 4, 'I-Procedure': 5, 'O': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "task = \"ner\"\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    label_all_tokens = True\n",
    "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == '0':\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label2id[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(label2id)\n",
    "\n",
    "train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "validation_tokenized_datasets = validation_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19a26d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35691/2251306310.py:16: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 231\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 120\n",
      "  Number of trainable parameters = 277458439\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhealth_xlmr\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 16:36:30.356160: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-11 16:36:30.527830: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-11 16:36:30.527869: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/space/projects/health_xlmr/repo_2804/xlmroberta_based/wandb/run-20231211_163619-aibijdao</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/health_xlmr/huggingface/runs/aibijdao\" target=\"_blank\">atomic-dawn-92</a></strong> to <a href=\"https://wandb.ai/health_xlmr/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 01:38, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.663505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.623420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.592327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.559209</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.042017</td>\n",
       "      <td>0.073801</td>\n",
       "      <td>0.842857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.527341</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.079832</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.847768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.515644</td>\n",
       "      <td>0.532710</td>\n",
       "      <td>0.239496</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.861607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.495981</td>\n",
       "      <td>0.511278</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.366577</td>\n",
       "      <td>0.860268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.500572</td>\n",
       "      <td>0.472393</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.384040</td>\n",
       "      <td>0.861161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 16\n",
      "/gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/space/home/shuva/miniconda3/envs/roberta/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "2023-12-11 16:36:44.645430: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-11 16:36:44.645830: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-11 16:36:44.645851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to xlmr_gpt4_batch05_250.model\n",
      "Configuration saved in xlmr_gpt4_batch05_250.model/config.json\n",
      "Model weights saved in xlmr_gpt4_batch05_250.model/pytorch_model.bin\n",
      "tokenizer config file saved in xlmr_gpt4_batch05_250.model/tokenizer_config.json\n",
      "Special tokens file saved in xlmr_gpt4_batch05_250.model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "results_collection = []\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    \n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    results_collection.append(results)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n",
    "    \n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "    eval_dataset=validation_tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model('xlmr_gpt4_batch05_250.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeeb5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, ner_tags. If tokens, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 300\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Drug': {'precision': 0.5543318649045521,\n",
       "  'recall': 0.6328583403185247,\n",
       "  'f1': 0.5909980430528375,\n",
       "  'number': 1193},\n",
       " 'Family': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 56},\n",
       " 'Procedure': {'precision': 0.2830687830687831,\n",
       "  'recall': 0.1096311475409836,\n",
       "  'f1': 0.15805022156573115,\n",
       "  'number': 1952},\n",
       " 'overall_precision': 0.45750708215297453,\n",
       " 'overall_recall': 0.302717900656045,\n",
       " 'overall_f1': 0.3643542019176537,\n",
       " 'overall_accuracy': 0.93797033950239}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(test_tokenized_datasets)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list_test[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adbe075c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Drug       0.55      0.63      0.59      1193\n",
      "      Family       0.00      0.00      0.00        56\n",
      "   Procedure       0.28      0.11      0.16      1952\n",
      "\n",
      "   micro avg       0.46      0.30      0.36      3201\n",
      "   macro avg       0.28      0.25      0.25      3201\n",
      "weighted avg       0.38      0.30      0.32      3201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels, true_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
